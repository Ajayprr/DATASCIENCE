{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to long-short term memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation through time (BPTT) is a training algorithm used to update weights in recurrennt neural networks. like LSTM\n",
    "    \n",
    "    To effectively frame sequence prediction problems for recurrent neural networks ,we must have a strong conceptual understanding of what BPTT is doing and how configurable variation like Truncated Backpropagationn through time will affect the skill, stability and speed when training your network ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@we figure out\n",
    "\n",
    "-> What BPTT is and how it relates to the Backpropagation training algorithm used by Multilayer perceptron networks\n",
    "\n",
    "-> The motivations that lead to the need of truncated BPTT ,the mostly widely used variant in deep learning for training LSTMs.\n",
    "\n",
    "-> A notation for thinking about how to congigure Truncated BPTT and canonicaly configurations used in research and by deep learning libraries.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Training Algorithm\n",
    "\n",
    "Backpropagation refers two things:\n",
    "\n",
    "    1.The mathematical method used to calculate derivatives and an application of the derivative chain rule.\n",
    "    \n",
    "    2.The training algorithm for updating network weights to minimize error.\n",
    "    \n",
    " The goal of the BPTT training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs campared to some expected output in response to corresponding inputs.\n",
    " \n",
    " It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made.\n",
    " \n",
    "The regular algorithm is as follows:\n",
    "\n",
    "             a) Present a training input pattern and propagate it through the network to get an output.\n",
    "             \n",
    "             b) Compare the predicted outputs to the expected outputs and calculate the error.\n",
    "             \n",
    "             c) Calculate the derivatives of the error with respect to the network weights.\n",
    "             \n",
    "             d) Adjust the weights to minimize the error.\n",
    "             \n",
    "             e) Repeat.\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Backpropagation Through Time (BPTT)\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A recurrent neural network is shown one input each timestep and predicts one output.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "1. Present a sequence of timesteps of input and output pairs to the network.\n",
    "2. Unroll the network then calculate and accumulate errors across each timestep.\n",
    "3. Roll-up the network and update weights.\n",
    "4. Repeat.\n",
    "\n",
    "BPTT can be computationally expensive as the number of timesteps increases.\n",
    "\n",
    "If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode (go to zero or overflow) and make slow learning and model skill noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Backpropagation Through Time\n",
    "Truncated Backpropagation Through Time, or TBPTT, is a modified version of the BPTT training algorithm for recurrent neural networks where the sequence is processed one timestep at a time and periodically (k1 timesteps) the BPTT update is performed back for a fixed number of timesteps (k2 timesteps).\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "1. Present a sequence of k1 timesteps of input and output pairs to the network.\n",
    "2. Unroll the network then calculate and accumulate errors across k2 timesteps.\n",
    "3. Roll-up the network and update weights.\n",
    "4. Repeat\n",
    "\n",
    "The TBPTT algorithm requires the consideration of two parameters:\n",
    "\n",
    "* **k1**: The number of forward-pass timesteps between updates. Generally, this influences how slow or fast training will be, given how often weight updates are performed.\n",
    "* **k2**: The number of timesteps to which to apply BPTT. Generally, it should be large enough to capture the temporal structure in the problem for the network to learn. Too large a value results in vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can borrow the notation from Williams and Peng and refer to different TBPTT configurations as TBPTT(k1, k2).\n",
    "\n",
    "Using this notation, we can define some standard or common approaches:\n",
    "\n",
    "Note, here n refers to the total number of timesteps in the sequence:\n",
    "\n",
    "* **TBPTT(n,n)**: Updates are performed at the end of the sequence across all timesteps in the sequence (e.g. classical BPTT).\n",
    "* **TBPTT(1,n)**: timesteps are processed one at a time followed by an update that covers all timesteps seen so far (e.g. classical TBPTT by Williams and Peng).\n",
    "* **TBPTT(k1,1)**: The network likely does not have enough temporal context to learn, relying heavily on internal state and inputs.\n",
    "* **TBPTT(k1,k2), where k1<k2<n**: Multiple updates are performed per sequence which can accelerate training.\n",
    "* **TBPTT(k1,k2), where k1=k2**: A common configuration where a fixed number of timesteps are used for both forward and backward-pass timesteps (e.g. 10s to 100s).\n",
    "\n",
    "We can see that all configurations are a variation on TBPTT(n,n) that essentially attempt to approximate the same effect with perhaps faster training and more stable results.\n",
    "\n",
    "Canonical TBPTT reported in papers may be considered TBPTT(k1,k2), where k1=k2=h and h<=n, and where the chosen parameter is small (tens to hundreds of timesteps).\n",
    "\n",
    "In libraries like TensorFlow and Keras, things look similar and h defines the vectorized fixed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this post, we discovered the Backpropagation Through Time for training recurrent neural networks.\n",
    "\n",
    "Specifically, we learned:\n",
    "\n",
    "* What Backpropagation Through Time is and how it relates to the Backpropagation training algorithm used by Multilayer Perceptron networks.\n",
    "* The motivations that lead to the need for Truncated Backpropagation Through Time, the most widely used variant in deep learning for training LSTMs.\n",
    "* A notation for thinking about how to configure Truncated Backpropagation Through Time and the canonical configurations used in research and by deep learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention in Long Short-Term Memory Recurrent Neural Networks\n",
    "The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.\n",
    "\n",
    "A limitation of the architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences.\n",
    "\n",
    "Here! we will discover the attention mechanism for recurrent neural networks that seeks to overcome this limitation.\n",
    "\n",
    "After reading this post, we will know:\n",
    "\n",
    "* The limitation of the encode-decoder architecture and the fixed-length internal representation.\n",
    "* The attention mechanism to overcome the limitation that allows the network to learn where to pay attention in the input sequence for each item in the output sequence.\n",
    "* 5 applications of the attention mechanism with recurrent neural networks in domains such as text translation, speech recognition, and more.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem With Long Sequences\n",
    "The encoder-decoder recurrent neural network is an architecture where one set of LSTMs learn to encode input sequences into a fixed-length internal representation, and second set of LSTMs read the internal representation and decode it into an output sequence.\n",
    "\n",
    "This architecture has shown state-of-the-art results on difficult sequence prediction problems like text translation and quickly became the dominant approach.\n",
    "\n",
    "The encoder-decoder architecture still achieves excellent results on a wide range of problems. Nevertheless, it suffers from the constraint that all input sequences are forced to be encoded to a fixed length internal vector.\n",
    "\n",
    "This is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems.\n",
    "\n",
    "## Attention within Sequences\n",
    "Attention is the idea of freeing the encoder-decoder architecture from the fixed-length internal representation.\n",
    "\n",
    "This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence.\n",
    "\n",
    "Put another way, each item in the output sequence is conditional on selective items in the input sequence.\n",
    "\n",
    "This increases the computational burden of the model, but results in a more targeted and better-performing model.\n",
    "\n",
    "In addition, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.\n",
    "\n",
    "## Problem with Large Images\n",
    "Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images.\n",
    "\n",
    "As a result, a series of glimpses can be taken of a large image to formulate an approximate impression of the image before making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Examples of Attention in Sequence Prediction\n",
    "This section provides some specific examples of how attention is used for sequence prediction with recurrent neural networks.\n",
    "\n",
    "### 1. Attention in Text Translation\n",
    "The motivating example mentioned above is text translation.\n",
    "\n",
    "Given an input sequence of a sentence in French, translate and output a sentence in English. Attention is used to pay attention to specific words in the input sequence for each word in the output sequence.\n",
    "\n",
    "![attention-text-translation](Attentional-Interpretation-of-French-to-English-Translation.png)\n",
    "\n",
    "### 2. Attention in Image Descriptions\n",
    "Different from the glimpse approach, the sequence-based attentional mechanism can be applied to computer vision problems to help get an idea of how to best use the convolutional neural network to pay attention to images when outputting a sequence, such as a caption.\n",
    "\n",
    "Given an input of an image, output an English description of the image. Attention is used to pay focus on different parts of the image for each word in the output sequence.\n",
    "\n",
    "![attention-image-description](Attentional-Interpretation-of-Output-Words-to-Specific-Regions-on-the-Input-Images.png)\n",
    "\n",
    "### 3. Attention in Entailment\n",
    "Given a premise scenario and a hypothesis about the scenario in English, output whether the premise contradicts, is not related, or entails the hypothesis.\n",
    "\n",
    "For example:\n",
    "\n",
    "* premise: “*A wedding party taking pictures*“\n",
    "* hypothesis: “*Someone got married*“\n",
    "\n",
    "Attention is used to relate each word in the hypothesis to words in the premise, and vise-versa.\n",
    "\n",
    "![attention-entailment](Attentional-Interpretation-of-Premise-Words-to-Hypothesis-Words.png)\n",
    "\n",
    "### 4. Attention in Speech Recognition\n",
    "Given an input sequence of English speech snippets, output a sequence of phonemes.\n",
    "\n",
    "Attention is used to relate each phoneme in the output sequence to specific frames of audio in the input sequence.\n",
    "\n",
    "![attention-speech-recognition](Attentional-Interpretation-of-Output-Phoneme-Location-to-Input-Frames-of-Audio.png)\n",
    "\n",
    "### 5. Attention in Text Summarization\n",
    "Given an input sequence of an English article, output a sequence of English words that summarize the input.\n",
    "\n",
    "Attention is used to relate each word in the output summary to specific words in the input document.\n",
    "\n",
    "![attention-text-summarization](Attentional-Interpretation-of-Words-in-the-Input-Document-to-the-Output-Summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.\n",
    "* That attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.\n",
    "* That the approach has been used across different types sequence prediction problems include text translation, speech recognition, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Long Short-Term Memory Networks\n",
    "Input with spatial structure, like images, cannot be modeled easily with the standard Vanilla LSTM.\n",
    "\n",
    "The CNN Long Short-Term Memory Network or CNN LSTM for short is an LSTM architecture specifically designed for sequence prediction problems with spatial inputs, like images or videos.\n",
    "\n",
    "In this post, you will discover the CNN LSTM architecture for sequence prediction.\n",
    "\n",
    "After completing this post, you will know:\n",
    "\n",
    "* About the development of the CNN LSTM model architecture for sequence prediction.\n",
    "* Examples of the types of problems to which the CNN LSTM model is suited.\n",
    "* How to implement the CNN LSTM architecture in Python with Keras.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LSTM Architecture\n",
    "The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
    "\n",
    "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos). Specifically, the problems of:\n",
    "\n",
    "* **Activity Recognition**: Generating a textual description of an activity demonstrated in a sequence of images.\n",
    "* **Image Description**: Generating a textual description of a single image.\n",
    "* **Video Description**: Generating a textual description of a sequence of images.\n",
    "\n",
    "This architecture was originally referred to as a Long-term Recurrent Convolutional Network or LRCN model, although we will use the more generic name “CNN LSTM” to refer to LSTMs that use a CNN as a front end in this lesson.\n",
    "\n",
    "This architecture is used for the task of generating textual descriptions of images. Key is the use of a CNN that is pre-trained on a challenging image classification task that is re-purposed as a feature extractor for the caption generating problem.\n",
    "\n",
    "This architecture has also been used on speech recognition and natural language processing problems where CNNs are used as feature extractors for the LSTMs on audio and textual input data.\n",
    "\n",
    "This architecture is appropriate for problems that:\n",
    "\n",
    "* Have spatial structure in their input such as the 2D structure or pixels in an image or the 1D structure of words in a sentence, paragraph, or document.\n",
    "* Have a temporal structure in their input such as the order of images in a video or words in text, or require the generation of output with temporal structure such as words in a textual description.\n",
    "\n",
    "![cnn-lstm](Convolutional-Neural-Network-Long-Short-Term-Memory-Network-Archiecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN LSTM in Keras\n",
    "We can define a CNN LSTM model to be trained jointly in Keras.\n",
    "\n",
    "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output.\n",
    "\n",
    "It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps.\n",
    "\n",
    "Let’s take a look at both of these sub models in the context of a sequence of 2D inputs which we will assume are images.\n",
    "\n",
    "### CNN Model\n",
    "As a refresher, we can define a 2D convolutional network as comprised of Conv2D and MaxPooling2D layers ordered into a stack of the required depth.\n",
    "\n",
    "The Conv2D will interpret snapshots of the image (e.g. small squares) and the polling layers will consolidate or abstract the interpretation.\n",
    "\n",
    "For example, the snippet below expects to read in 10×10 pixel images with 1 channel (e.g. black and white). The Conv2D will read the image in 2×2 snapshots and output one new 10×10 interpretation of the image. The MaxPooling2D will pool the interpretation into 2×2 blocks reducing the output to a 5×5 consolidation. The Flatten layer will take the single 5×5 map and transform it into a 25-element vector ready for some other layer to deal with, such as a Dense for outputting a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-97f0495fad8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(1, (2,2), activation='relu', padding='same', input_shape=(10,10,1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense for image classification and other computer vision tasks.\n",
    "\n",
    "### LSTM Model\n",
    "The CNN model above is only capable of handling a single image, transforming it from input pixels into an internal matrix or vector representation.\n",
    "\n",
    "We need to repeat this operation across multiple images and allow the LSTM to build up internal state and update weights using BPTT across a sequence of the internal vector representations of input images.\n",
    "\n",
    "The CNN could be fixed in the case of using an existing pre-trained model like VGG for feature extraction from images. The CNN may not be trained, and we may wish to train it by backpropagating error from the LSTM across multiple input images to the CNN model.\n",
    "\n",
    "In both of these cases, conceptually there is a single CNN model and a sequence of LSTM models, one for each time step. We want to apply the CNN model to each input image and pass on the output of each input image to the LSTM as a single time step.\n",
    "\n",
    "We can achieve this by wrapping the entire CNN input model (one layer or more) in a TimeDistributed layer. This layer achieves the desired outcome of applying the same layer or layers multiple times. In this case, applying it multiple times to multiple input time steps and in turn providing a sequence of “image interpretations” or “image features” to the LSTM model to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-affbbbba4251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add(TimeDistributed(...))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the two elements of the model; let’s put them together.\n",
    "\n",
    "### CNN LSTM Model\n",
    "We can define a CNN LSTM model in Keras by first defining the CNN layer or layers, wrapping them in a TimeDistributed layer and then defining the LSTM and output layers.\n",
    "\n",
    "We have two ways to define the model that are equivalent and only differ as a matter of taste.\n",
    "\n",
    "You can define the CNN model first, then add it to the LSTM model by wrapping the entire sequence of CNN layers in a TimeDistributed layer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(...))\n",
    "cnn.add(MaxPooling2D(...))\n",
    "cnn.add(Flatten())\n",
    "# define LSTM model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(cnn, ...))\n",
    "model.add(LSTM(..))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate, and perhaps easier to read, approach is to wrap each layer in the CNN model in a TimeDistributed layer when adding it to the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv2D(...))\n",
    "model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define LSTM model\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of this second approach is that all of the layers appear in the model summary and as such is preferred for now.\n",
    "\n",
    "You can choose the method that you prefer.\n",
    "\n",
    "## Summary\n",
    "In this post, you discovered the CNN LSTN model architecture.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* About the development of the CNN LSTM model architecture for sequence prediction.\n",
    "* Examples of the types of problems to which the CNN LSTM model is suited.\n",
    "* How to implement the CNN LSTM architecture in Python with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
